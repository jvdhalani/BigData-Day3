{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Spark SQL** - Spark’s interface for working with structured and semistructured data. \n",
    "\n",
    "- **Structured data** is any data that has a schema—that is, a known set of fields for each record.\n",
    "\n",
    "- Spark SQL lets you query structured data inside Spark programs, using either **SQL** or a familiar **DataFrame API**. Usable in Java, Scala, Python and R.\n",
    "\n",
    "- Spark SQL is use to execute SQL queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In Apache Spark, **a DataFrame is a distributed collection of rows under named columns.**\n",
    "\n",
    "- In simple terms, it is same as a table in relational database or an Excel sheet with Column headers. It also shares some common characteristics with RDD:\n",
    "\n",
    "    - **Immutable in nature :** We can create DataFrame / RDD once but can’t change it. And we can transform a DataFrame / RDD  after applying transformations.\n",
    "    - **Lazy Evaluations:** Which means that a task is not executed until an action is performed.\n",
    "\n",
    "    - **Distributed:** RDD and DataFrame both are distributed in nature.\n",
    "\n",
    "- When running SQL from within another programming language the results will be returned as a DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why DataFrames are Useful ?\n",
    "\n",
    "After learning about pandas dataframes, you must be aware of many advantages that Dataframes provides us with. But the question is, what additional advantages Dataframes in spark provides us with?\n",
    "\n",
    "- DataFrames are designed for processing large collection of structured or semi-structured data.\n",
    "\n",
    "- Observations in Spark DataFrame are organised under named columns, which helps Apache Spark to understand the schema of a DataFrame. This helps Spark optimize execution plan on these queries.\n",
    "\n",
    "- DataFrame in Apache Spark has the ability to handle petabytes of data.\n",
    "\n",
    "- DataFrame has a support for wide range of data format and sources.\n",
    "\n",
    "- It has API support for different languages like Python, R, Scala, Java.\n",
    "\n",
    "- Like our RDDs are distibuted across machines in a cluster similarly dataframes provides us with distributed computation capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As a general rule of thumb, one should consider an alternative to Pandas whenever the data set has more than 10,000,000 rows which, depending on the number of columns and data types, translates to about 5-10 GB of memory usage. At that point PySpark might be an option for you that does the job**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check if SparkContext is running!**\n",
    "\n",
    "**Recall - Why do we need a SparkContext running?**\n",
    "\n",
    "- First step, in any Apache programming is to create a SparkContext. SparkContext is required when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.17.9.210:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, start a **SQLContext**. Now, Why **SQLContext**?\n",
    "\n",
    "- The entry point into all relational functionality in Spark is the SQLContext class.\n",
    "- Basically it is must to have SQLContext in order to perform SQL related operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into spark versions<2.0\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Into spark version>2.0\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark dataframe basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: `getOrCreate()`-** Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a DataFrame ?\n",
    "A DataFrame in Apache Spark can be created in multiple ways:\n",
    "\n",
    "It can be created using different data formats. For example:\n",
    "1. Loading data from Existing RDD.\n",
    "2. Loading the data from JSON, CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.  Creating DataFrame from RDD\n",
    "\n",
    "One can easily create a dataframe out of a List of tuples. Steps can be as follows:\n",
    "\n",
    "1. Create a list of tuples. Each tuple contains name of a person with age.\n",
    "2. Create a RDD from the list above.\n",
    "3. Convert each tuple to a row.\n",
    "4. Create a DataFrame by applying createDataFrame on RDD with the help of sqlContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [('Sam',25, 'M'),('Jalfaizy',22, 'F'),('Tom',20, 'M'),('Nicky',26, 'F'),('Wrick', 30, 'M')]\n",
    "rdd = sc.parallelize(l)\n",
    "people = rdd.map(lambda x: Row(name=x[0], age=int(x[1]), Gender=x[2]))\n",
    "schemaPeople = sqlContext.createDataFrame(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(schemaPeople)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating the DataFrame from external file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Dataset\n",
    "\n",
    "### Context\n",
    "\n",
    "H-1B visas are a category of employment-based, non-immigrant visas for temporary foreign workers in the United States. For a foreign national to apply for H1-B visa, a US employer must offer them a job and submit a petition for a H-1B visa to the US immigration department. This is also the most common visa status applied for and held by international students once they complete college or higher education and begin working in a full-time position.\n",
    "\n",
    "This dataset contains H-1B petition data. The columns in the dataset include case status, employer name, worksite coordinates, job title, prevailing wage, occupation code, and year filed.\n",
    "\n",
    "For more information on individual columns, refer to the column metadata. A detailed description of the underlying raw dataset is available in an [official data dictionary](https://www.foreignlaborcert.doleta.gov/docs/Performance_Data/Disclosure/FY15-FY16/H-1B_FY16_Record_Layout.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my tutorial session, I'll use the data file `h1b_sample.csv` but would strongly recommed that you use the data file `h1b_learners.csv` to do the hands-on practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('../data/h1b_learners.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we read data into the SQLContext object, Spark:\n",
    "\n",
    "- Instantiates a Spark DataFrame object\n",
    "- Infers the schema from the data and associates it with the DataFrame\n",
    "- Reads in the data and distributes it across clusters (if multiple clusters are available)\n",
    "- Returns the DataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the **schema for the DataFrame** we created out of our dataset. For this, we can call `printSchema()` method on our dataframe. This will provide us the datatype of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CASE_STATUS: string (nullable = true)\n",
      " |-- EMPLOYER_NAME: string (nullable = true)\n",
      " |-- SOC_NAME: string (nullable = true)\n",
      " |-- JOB_TITLE: string (nullable = true)\n",
      " |-- FULL_TIME_POSITION: string (nullable = true)\n",
      " |-- PREVAILING_WAGE: string (nullable = true)\n",
      " |-- YEAR: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`show()`** method on a DataFrame can give us a quick look on rows of the DataFame. Use `show()` to display 5 Rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|          SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|         OMD USA LLC|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|       108493.0|2016|\n",
      "|          CERTIFIED|THE GLOBAL ALLIAN...|MARKETING MANAGERS|SENIOR VP, MARKET...|                 Y|       179774.0|2016|\n",
      "|          CERTIFIED|THE GATORADE COMPANY|MARKETING MANAGERS|SENIOR MANAGER, S...|                 Y|       123386.0|2016|\n",
      "|          CERTIFIED|BOOMERANG COMMERC...|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 Y|       163877.0|2016|\n",
      "|          CERTIFIED|AMAZON WEB SERVIC...|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|CORNING INCORPORATED|MARKETING MANAGERS|COMPETITIVE INTEL...|                 Y|       123698.0|2016|\n",
      "|          CERTIFIED|          SNAP, INC.|MARKETING MANAGERS|DIRECTOR, BUSINES...|                 Y|       124197.0|2016|\n",
      "|          CERTIFIED|      ASTRAZENECA LP|MARKETING MANAGERS|ONCOLOGY INSIGHT ...|                 Y|       120786.0|2016|\n",
      "|          CERTIFIED|STARBUCKS COFFEE ...|MARKETING MANAGERS|    BRAND MANAGER II|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|NXSTAGE MEDICAL, ...|MARKETING MANAGERS|ASSOCIATE PRODUCT...|                 Y|        91312.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|       101587.0|2016|\n",
      "|          CERTIFIED|POSSIBLE WORLDWID...|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|        77106.0|2016|\n",
      "|          CERTIFIED|WAL-MART ASSOCIAT...|MARKETING MANAGERS|SENIOR DIRECTOR, ...|                 Y|       179733.0|2016|\n",
      "|          CERTIFIED|CARNEGIE MELLON U...|MARKETING MANAGERS|DIRECTOR OF MARKE...|                 Y|        71718.0|2016|\n",
      "|          WITHDRAWN| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|KOYO BEARINGS NOR...|MARKETING MANAGERS|      LAUNCH MANAGER|                 Y|       102211.0|2016|\n",
      "|          CERTIFIED|UNIVERSITY OF HAWAII|MARKETING MANAGERS|PUBLIC INFO, PUBL...|                 N|        50352.0|2016|\n",
      "|          CERTIFIED| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|         ACCUEN INC.|MARKETING MANAGERS|  DIRECTOR, INSIGHTS|                 N|        69618.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|        90002.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|TECH IP DEVELOPME...|                 Y|        90002.0|2016|\n",
      "|          CERTIFIED|        VMWARE, INC.|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       167268.0|2016|\n",
      "|          CERTIFIED|     UST GLOBAL INC.|MARKETING MANAGERS|SENIOR ACCOUNT DI...|                 Y|       106330.0|2016|\n",
      "|          CERTIFIED|  SPECTRAMD USA INC.|MARKETING MANAGERS|SENIOR CLINICAL A...|                 Y|        89440.0|2016|\n",
      "|          CERTIFIED|  CVS PHARMACY, INC.|MARKETING MANAGERS|MERCHANDISING DIR...|                 Y|       156957.0|2016|\n",
      "|          CERTIFIED|  SEGA NETWORKS INC.|MARKETING MANAGERS|SENIOR BUSINESS D...|                 Y|       149656.0|2016|\n",
      "|          CERTIFIED| B/E AEROSPACE, INC.|MARKETING MANAGERS|MANAGER, MARKETIN...|                 Y|        81744.0|2016|\n",
      "|          CERTIFIED|            GOGO LLC|MARKETING MANAGERS|     PRODUCT MANAGER|                 Y|        94245.0|2016|\n",
      "|          CERTIFIED|     GRAMMARLY, INC.|MARKETING MANAGERS|     PRODUCT MANAGER|                 Y|       222976.0|2016|\n",
      "|          CERTIFIED|      TOCA BOCA INC.|MARKETING MANAGERS|DIRECTOR OF BUSIN...|                 Y|       113006.0|2016|\n",
      "|          CERTIFIED|    CITTA NUOVA, LLC|MARKETING MANAGERS|   MARKETING MANAGER|                 Y|        72675.0|2016|\n",
      "|          CERTIFIED|   KOVER CORPORATION|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 N|        60777.6|2016|\n",
      "|          CERTIFIED|    CYBERONICS, INC.|MARKETING MANAGERS|DIRECTOR, BUSINES...|                 Y|       127899.0|2016|\n",
      "|          CERTIFIED|GOLDMAN SACHS BAN...|MARKETING MANAGERS|VICE PRESIDENT/IN...|                 Y|       185224.0|2016|\n",
      "|          CERTIFIED|    FETCH MEDIA INC.|MARKETING MANAGERS|GLOBAL ACCOUNT DI...|                 Y|       113006.0|2016|\n",
      "|          CERTIFIED|     GILT CITY, INC.|MARKETING MANAGERS|DIRECTOR, STRATEG...|                 Y|       147243.0|2016|\n",
      "|          CERTIFIED|SAMSUNG ELECTRONI...|MARKETING MANAGERS|DIRECTOR OF CUSTO...|                 Y|       170082.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|      GT NEXUS, INC.|MARKETING MANAGERS|PRODUCT MANAGER, ...|                 Y|       135034.0|2016|\n",
      "|          CERTIFIED|ORACLE AMERICA, INC.|MARKETING MANAGERS|PRODUCT MANAGER/S...|                 Y|       170082.0|2016|\n",
      "|          CERTIFIED|        ARKENA, INC.|MARKETING MANAGERS|SENIOR BUSINESS D...|                 Y|       113048.0|2016|\n",
      "|          CERTIFIED|HONEYWELL INTERNA...|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       112694.0|2016|\n",
      "|          CERTIFIED|JAM INDUSTRIES US...|MARKETING MANAGERS|MARKETING MANAGER...|                 Y|        78208.0|2016|\n",
      "|          CERTIFIED|  CITRON PHARMA, LLC|MARKETING MANAGERS|DIRECTOR OF MARKE...|                 Y|       147243.0|2016|\n",
      "|          CERTIFIED|   OCHO DIGITAL, INC|MARKETING MANAGERS|BRAND MARKETING M...|                 N|        55182.0|2016|\n",
      "|          CERTIFIED|BE CONTENT GROUP,...|MARKETING MANAGERS|   MARKETING MANAGER|                 N|        60778.0|2016|\n",
      "|          CERTIFIED|SYNTEL CONSULTING...|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 Y|       114088.0|2016|\n",
      "|          CERTIFIED|     J&J CORPORATION|MARKETING MANAGERS|VICE PRESIDENT OF...|                 N|        68370.0|2016|\n",
      "|          CERTIFIED|MECHOSHADE SYSTEM...|MARKETING MANAGERS|   MARKETING MANAGER|                 Y|       147243.2|2016|\n",
      "|          CERTIFIED|   OCHO DIGITAL, INC|MARKETING MANAGERS|BRAND MARKETING M...|                 N|        55182.0|2016|\n",
      "|          CERTIFIED|BMO HARRIS BANK, ...|MARKETING MANAGERS|MANAGER, MARKETIN...|                 Y|        94245.0|2016|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "Let's have statistical view of our dataframe.\n",
    "\n",
    "We can use `describe(*cols)` method on a dataframe to compute statistics for numeric and string columns.\n",
    "\n",
    "This include count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "|summary|CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|   PREVAILING_WAGE|  YEAR|\n",
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "|  count|       5000|                5000|                5000|                5000|              5000|              5000|  5000|\n",
      "|   mean|       null|                null|                null|                null|              null|179312.78471200023|2016.0|\n",
      "| stddev|       null|                null|                null|                null|              null| 4653266.403375367|   0.0|\n",
      "|    min|  CERTIFIED|1-800-FLOWERS.COM...|ADMINISTRATIVE SE...|(UI) USER INTERFA...|                 N|               0.0|  2016|\n",
      "|    max|  WITHDRAWN|          ZYNGA INC.|       SALES MANGERS|WORLDWIDE PARTNER...|                 Y|           99900.0|  2016|\n",
      "+-------+-----------+--------------------+--------------------+--------------------+------------------+------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pandas, we used the `head()` method to return the first n rows. This is one of the differences between the DataFrame implementations. Instead of returning a nicely formatted table of values, the head() method in Spark returns a list of row objects. Spark needs to return row objects for certain methods, such as head(), collect() and take().\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'OMD USA LLC', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'ASSOCIATE DIRECTOR, DIGITAL ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'108493.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'THE GLOBAL ALLIANCE FOR TB DRUG DEVELOPMENT INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR VP, MARKET ACCESS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'179774.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'THE GATORADE COMPANY', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR MANAGER, SOCIAL MEDIA MARKETING & ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'123386.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'BOOMERANG COMMERCE, INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'BUSINESS DEVELOPMENT MANAGER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'163877.0', YEAR=u'2016'),\n",
       " Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'AMAZON WEB SERVICES, INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR PRODUCT MANAGER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'115565.0', YEAR=u'2016')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, print the first row out the five fetched rows. Then print the `EMPLOYER_NAME` for the first row entry.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'OMD USA LLC', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'ASSOCIATE DIRECTOR, DIGITAL ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'108493.0', YEAR=u'2016')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'OMD USA LLC'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)[0].EMPLOYER_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(CASE_STATUS=u'CERTIFIED-WITHDRAWN', EMPLOYER_NAME=u'OMD USA LLC', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'ASSOCIATE DIRECTOR, DIGITAL ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'108493.0', YEAR=u'2016'), Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'THE GLOBAL ALLIANCE FOR TB DRUG DEVELOPMENT INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR VP, MARKET ACCESS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'179774.0', YEAR=u'2016'), Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'THE GATORADE COMPANY', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR MANAGER, SOCIAL MEDIA MARKETING & ANALYTICS', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'123386.0', YEAR=u'2016'), Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'BOOMERANG COMMERCE, INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'BUSINESS DEVELOPMENT MANAGER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'163877.0', YEAR=u'2016'), Row(CASE_STATUS=u'CERTIFIED', EMPLOYER_NAME=u'AMAZON WEB SERVICES, INC.', SOC_NAME=u'MARKETING MANAGERS', JOB_TITLE=u'SENIOR PRODUCT MANAGER', FULL_TIME_POSITION=u'Y', PREVAILING_WAGE=u'115565.0', YEAR=u'2016')]\n",
      "ASSOCIATE DIRECTOR, DIGITAL ANALYTICS\n",
      "SENIOR VP, MARKET ACCESS\n",
      "SENIOR MANAGER, SOCIAL MEDIA MARKETING & ANALYTICS\n",
      "BUSINESS DEVELOPMENT MANAGER\n",
      "SENIOR PRODUCT MANAGER\n"
     ]
    }
   ],
   "source": [
    "first_five = df.head(5)\n",
    "print(first_five)\n",
    "for each_element in first_five:\n",
    "    print each_element.JOB_TITLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting columns\n",
    "In pandas, we pass a string into a single pair of brackets ([]) to select an individual column, and pass in a list to select multiple columns. For example:\n",
    "\n",
    "#### Pandas DataFrame\n",
    "df['age']\n",
    "\n",
    "df[['age', 'males']]\n",
    "\n",
    "Spark also allows us to use bracket notation. Pass in a list of string objects with column name to select any column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the age value for first five employees in the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[YEAR: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes being lazily evaluated like RDDs will only display the results of an operation when we call any action upon it. We can call the show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|YEAR|\n",
      "+----+\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "|2016|\n",
      "+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('YEAR').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Display Employer Name with their case status.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+\n",
      "|       EMPLOYER_NAME|        CASE_STATUS|\n",
      "+--------------------+-------------------+\n",
      "|         OMD USA LLC|CERTIFIED-WITHDRAWN|\n",
      "|THE GLOBAL ALLIAN...|          CERTIFIED|\n",
      "|THE GATORADE COMPANY|          CERTIFIED|\n",
      "|BOOMERANG COMMERC...|          CERTIFIED|\n",
      "|AMAZON WEB SERVIC...|          CERTIFIED|\n",
      "|CORNING INCORPORATED|          CERTIFIED|\n",
      "|          SNAP, INC.|          CERTIFIED|\n",
      "|      ASTRAZENECA LP|          CERTIFIED|\n",
      "|STARBUCKS COFFEE ...|          CERTIFIED|\n",
      "|NXSTAGE MEDICAL, ...|          CERTIFIED|\n",
      "|         MAYO CLINIC|          CERTIFIED|\n",
      "|POSSIBLE WORLDWID...|          CERTIFIED|\n",
      "|WAL-MART ASSOCIAT...|          CERTIFIED|\n",
      "|CARNEGIE MELLON U...|          CERTIFIED|\n",
      "| FARIA SYSTEMS, INC.|          WITHDRAWN|\n",
      "|KOYO BEARINGS NOR...|          CERTIFIED|\n",
      "|UNIVERSITY OF HAWAII|          CERTIFIED|\n",
      "| FARIA SYSTEMS, INC.|          CERTIFIED|\n",
      "|         ACCUEN INC.|          CERTIFIED|\n",
      "|         MAYO CLINIC|          CERTIFIED|\n",
      "+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hint: Use select() to display required columns\n",
    "#df.select('EMPLOYER_NAME')\n",
    "df.select('EMPLOYER_NAME','CASE_STATUS').show()\n",
    "#df[['EMPLOYER_NAME','CASE_STATUS']].show()\n",
    "#df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the total number of rows in our dataframe. We can use count() to give us total number of rows in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop Rows containing NULL values**\n",
    "\n",
    "We can use `drop(how='any', thresh=None, subset=None)` method on our dataframe to drop rows with null values and return a new dataframe.\n",
    "\n",
    "**Parameters:**\t\n",
    "\n",
    "**how** – ‘any’ or ‘all’. If ‘any’, drop a row if it contains any nulls. If ‘all’, drop a row only if all its values are null.\n",
    "\n",
    "**thresh** – int, default None If specified, drop rows that have less than thresh non-null values. This overwrites the how parameter.\n",
    "\n",
    "**subset** – optional list of column names to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|            SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|UNIVERSITY OF MIC...|BIOCHEMISTS AND B...|POSTDOCTORAL RESE...|                 N|        36067.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GOODMAN NETWORKS,...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|       242674.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|PORTS AMERICA GRO...|    CHIEF EXECUTIVES|CHIEF PROCESS OFF...|                 Y|       193066.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GATES CORPORATION...|    CHIEF EXECUTIVES|REGIONAL PRESIDEN...|                 Y|       220314.0|2016|\n",
      "|          WITHDRAWN|PEABODY INVESTMEN...|    CHIEF EXECUTIVES|PRESIDENT MONGOLI...|                 Y|       157518.4|2016|\n",
      "|CERTIFIED-WITHDRAWN|BURGER KING CORPO...|    CHIEF EXECUTIVES|EXECUTIVE V P, GL...|                 Y|       225000.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|BT AND MK ENERGY ...|    CHIEF EXECUTIVES|CHIEF OPERATING O...|                 Y|        91021.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|GLOBO MOBILE TECH...|    CHIEF EXECUTIVES|CHIEF OPERATIONS ...|                 Y|       150000.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|  ESI COMPANIES INC.|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       127546.0|2016|\n",
      "|          WITHDRAWN|LESSARD INTERNATI...|    CHIEF EXECUTIVES|           PRESIDENT|                 Y|       154648.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|  H.J. HEINZ COMPANY|    CHIEF EXECUTIVES|CHIEF INFORMATION...|                 Y|       182978.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|DOW CORNING CORPO...|    CHIEF EXECUTIVES|VICE PRESIDENT AN...|                 Y|       163717.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|    ACUSHNET COMPANY|    CHIEF EXECUTIVES|   TREASURER AND COO|                 Y|       203860.8|2016|\n",
      "|CERTIFIED-WITHDRAWN|       BIOCAIR, INC.|    CHIEF EXECUTIVES|CHIEF COMMERCIAL ...|                 Y|       252637.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|NEWMONT MINING CO...|    CHIEF EXECUTIVES|        BOARD MEMBER|                 Y|       105914.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|        VRICON, INC.|    CHIEF EXECUTIVES|CHIEF FINANCIAL O...|                 Y|       153046.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|CARDIAC SCIENCE C...|  FINANCIAL MANAGERS|VICE PRESIDENT OF...|                 Y|        90834.0|2016|\n",
      "|CERTIFIED-WITHDRAWN|WESTFIELD CORPORA...|    CHIEF EXECUTIVES|GENERAL MANAGER, ...|                 Y|       164050.0|2016|\n",
      "|          CERTIFIED|      QUICKLOGIX LLC|    CHIEF EXECUTIVES|                 CEO|                 Y|       187200.0|2016|\n",
      "|          CERTIFIED|MCCHRYSTAL GROUP,...|    CHIEF EXECUTIVES|PRESIDENT, NORTHE...|                 Y|       241842.0|2016|\n",
      "+-------------------+--------------------+--------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.na.drop()\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace Null values\n",
    "**What if don't want to drop entire row but just replace the null values?**\n",
    "\n",
    "`fillna(value, subset=None)` enables us to replace null values in our dataframe. We can optionally specify the set of columns into which we want to replace nul values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace null values in all the columns\n",
    "df = df.fillna(0)\n",
    "# df.fillna(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace null values only in the Columns `CASE_STATUS` and `EMPLOYER_NAME`. \n",
    "\n",
    "Hint: Use `fillna(value, subset=None)` and specify required column names in the subset parameter. For example - `df.fillna(0, subset=['a', 'b'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Row is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-4821fc4f0ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCASE_STATUS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/javed/spark/spark-2.3.1-bin-hadoop2.7/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'__fields__'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"__from_dict__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Row is read-only\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Row is read-only"
     ]
    }
   ],
   "source": [
    "df.head(5)[0].CASE_STATUS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|        CASE_STATUS|       EMPLOYER_NAME|          SOC_NAME|           JOB_TITLE|FULL_TIME_POSITION|PREVAILING_WAGE|YEAR|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "|CERTIFIED-WITHDRAWN|         OMD USA LLC|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|       108493.0|2016|\n",
      "|          CERTIFIED|THE GLOBAL ALLIAN...|MARKETING MANAGERS|SENIOR VP, MARKET...|                 Y|       179774.0|2016|\n",
      "|          CERTIFIED|THE GATORADE COMPANY|MARKETING MANAGERS|SENIOR MANAGER, S...|                 Y|       123386.0|2016|\n",
      "|          CERTIFIED|BOOMERANG COMMERC...|MARKETING MANAGERS|BUSINESS DEVELOPM...|                 Y|       163877.0|2016|\n",
      "|          CERTIFIED|AMAZON WEB SERVIC...|MARKETING MANAGERS|SENIOR PRODUCT MA...|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|CORNING INCORPORATED|MARKETING MANAGERS|COMPETITIVE INTEL...|                 Y|       123698.0|2016|\n",
      "|          CERTIFIED|          SNAP, INC.|MARKETING MANAGERS|DIRECTOR, BUSINES...|                 Y|       124197.0|2016|\n",
      "|          CERTIFIED|      ASTRAZENECA LP|MARKETING MANAGERS|ONCOLOGY INSIGHT ...|                 Y|       120786.0|2016|\n",
      "|          CERTIFIED|STARBUCKS COFFEE ...|MARKETING MANAGERS|    BRAND MANAGER II|                 Y|       115565.0|2016|\n",
      "|          CERTIFIED|NXSTAGE MEDICAL, ...|MARKETING MANAGERS|ASSOCIATE PRODUCT...|                 Y|        91312.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|       101587.0|2016|\n",
      "|          CERTIFIED|POSSIBLE WORLDWID...|MARKETING MANAGERS|ASSOCIATE DIRECTO...|                 Y|        77106.0|2016|\n",
      "|          CERTIFIED|WAL-MART ASSOCIAT...|MARKETING MANAGERS|SENIOR DIRECTOR, ...|                 Y|       179733.0|2016|\n",
      "|          CERTIFIED|CARNEGIE MELLON U...|MARKETING MANAGERS|DIRECTOR OF MARKE...|                 Y|        71718.0|2016|\n",
      "|          WITHDRAWN| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|KOYO BEARINGS NOR...|MARKETING MANAGERS|      LAUNCH MANAGER|                 Y|       102211.0|2016|\n",
      "|          CERTIFIED|UNIVERSITY OF HAWAII|MARKETING MANAGERS|PUBLIC INFO, PUBL...|                 N|        50352.0|2016|\n",
      "|          CERTIFIED| FARIA SYSTEMS, INC.|MARKETING MANAGERS|MARKETING AND OPE...|                 Y|       110074.0|2016|\n",
      "|          CERTIFIED|         ACCUEN INC.|MARKETING MANAGERS|  DIRECTOR, INSIGHTS|                 N|        69618.0|2016|\n",
      "|          CERTIFIED|         MAYO CLINIC|MARKETING MANAGERS|NEW PRODUCTS AND ...|                 Y|        90002.0|2016|\n",
      "+-------------------+--------------------+------------------+--------------------+------------------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filterd = ['CASE_STATUS','EMPLOYER_NAME']\n",
    "df.fillna(0,subset=df_filterd).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the possible categories in the case status?\n",
    "\n",
    "`distinct()`: Returns a new DataFrame containing the distinct rows in this DataFrame.\n",
    "\n",
    "So next, select the `CASE_STATUS` column and apply `distint()` method on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|        CASE_STATUS|\n",
      "+-------------------+\n",
      "|          CERTIFIED|\n",
      "|CERTIFIED-WITHDRAWN|\n",
      "|          WITHDRAWN|\n",
      "|             DENIED|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('CASE_STATUS').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2467"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('EMPL').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine Distinct `CASE_STATUS` count for each `EMPLOYER_NAME`\n",
    "\n",
    "For example, determine how many visa applications are certified under the employer name `SAMSUNG ELECTRONICS`\n",
    "\n",
    "We can use `crosstab()` method to get this done. **crosstab(col1, col2)** computes a pair-wise frequency table of the given columns.\n",
    "\n",
    "**Parameters:**\t\n",
    "\n",
    "**col1** – The name of the first column. Distinct items will make the first item of each row.\n",
    "\n",
    "**col2** – The name of the second column. Distinct items will make the column names of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.crosstab('EMPLOYER_NAME', 'CASE_STATUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Determine the top Employers getting more visa applications into a Certified Status**\n",
    "\n",
    "Find out the top 10 companies having highest number of certified visa applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|     TEMPLE UNIVERSITY...|        2|                  0|     0|        0|\n",
      "|     SAMSUNG ELECTRONI...|        4|                  1|     0|        0|\n",
      "|     MERCURY INSURANCE...|        0|                  1|     0|        0|\n",
      "|              ACCUEN INC.|        1|                  0|     0|        0|\n",
      "|        AKDY IMPORTS, LLC|        1|                  0|     0|        0|\n",
      "|           SAFEGRAPH INC.|        2|                  0|     0|        0|\n",
      "|     PUMA NORTH AMERIC...|        1|                  0|     0|        0|\n",
      "|             CYIENT, INC.|        1|                  0|     0|        0|\n",
      "|     THE SHERWIN-WILLI...|        3|                  0|     1|        0|\n",
      "|     ITG SOFTWARE SOLU...|        0|                  1|     0|        0|\n",
      "|             ANTHEM, INC.|        3|                  0|     0|        0|\n",
      "|     GAVS TECHNOLOGIES...|        1|                  0|     0|        0|\n",
      "|     GLOBAL TRAVEL SOL...|        1|                  0|     0|        0|\n",
      "|     INTERACTIVE BROAD...|        4|                  0|     0|        0|\n",
      "|            POPSUGAR INC.|        1|                  0|     0|        0|\n",
      "|       BRIGHT MARKET, LLC|        1|                  0|     0|        0|\n",
      "|     NOBLE DRILLING SE...|        1|                  0|     0|        0|\n",
      "|               PROJECT:TF|        1|                  0|     0|        0|\n",
      "|           PEOPLEASE, LLC|        1|                  0|     0|        0|\n",
      "|              AASONN, LLC|        1|                  0|     0|        0|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------+------+---------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "| SUPPORT ENGINEER ...|        2|                  0|     0|        0|\n",
      "| DIVISION SALES MA...|        0|                  0|     0|        1|\n",
      "| INTERNATIONAL OPE...|        1|                  0|     2|        0|\n",
      "| IT ENTERPRISE/SAP...|        1|                  0|     0|        0|\n",
      "| INFRASTRUCTURE EN...|        1|                  0|     0|        0|\n",
      "| ASSOCIATE DIRECTO...|        1|                  0|     0|        0|\n",
      "| CONSULTING MANAGE...|        1|                  0|     0|        0|\n",
      "| VICE PRESIDENT OF...|        1|                  0|     0|        0|\n",
      "| MANAGER, INFRASTR...|        1|                  0|     0|        0|\n",
      "| DIRECTOR OF BUSIN...|       14|                  1|     0|        1|\n",
      "| DIRECTOR OF OPERA...|        1|                  0|     0|        0|\n",
      "| DIRECTOR, STATIST...|        1|                  0|     0|        0|\n",
      "| INTERNATIONAL BUS...|        1|                  0|     0|        1|\n",
      "| SENIOR COMPUTER S...|        1|                  0|     0|        0|\n",
      "| SENIOR TECHNICAL ...|        0|                  0|     0|        1|\n",
      "| VICE PRESIDENT - ...|        0|                  1|     0|        0|\n",
      "| VICE PRESIDENT OF...|        1|                  2|     0|        0|\n",
      "|    RESEARCH DIRECTOR|        0|                  1|     0|        0|\n",
      "| ASSOC. VP, IT APP...|        0|                  1|     0|        0|\n",
      "| SALES & MARKETING...|        1|                  0|     0|        0|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df.crosstab('JOB_TITLE', 'CASE_STATUS')\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------+------+---------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "|    MARKETING MANAGER|       67|                  1|     9|        2|\n",
      "|        SALES MANAGER|      146|                  5|     7|        9|\n",
      "| BUSINESS DEVELOPM...|       52|                  3|     4|        1|\n",
      "|   MARKETING DIRECTOR|       12|                  0|     2|        1|\n",
      "| ADMINISTRATIVE SE...|        4|                  0|     2|        0|\n",
      "| ADMINISTRATIVE SE...|       14|                  1|     2|        2|\n",
      "|      PROJECT MANAGER|      215|                 21|     2|        8|\n",
      "| VP, BUSINESS DEVE...|        3|                  0|     2|        0|\n",
      "| INTERNATIONAL OPE...|        1|                  0|     2|        0|\n",
      "| SOFTWARE DEVELOPM...|       51|                  1|     1|        2|\n",
      "| DIRECTOR OF MARKE...|        9|                  0|     1|        0|\n",
      "|      VP, ENGINEERING|        5|                  0|     1|        0|\n",
      "|      ACCOUNT MANAGER|       19|                  0|     1|        1|\n",
      "|          IT DIRECTOR|        6|                  0|     1|        0|\n",
      "| SENIOR PROJECT MA...|       13|                  1|     1|        1|\n",
      "| TECHNICAL PROJECT...|       13|                  6|     1|        0|\n",
      "|       OFFICE MANAGER|        6|                  0|     1|        0|\n",
      "|              MANAGER|      132|                  6|     1|        2|\n",
      "|      LEAD CONSULTANT|      121|                  0|     1|        1|\n",
      "|      PRODUCT MANAGER|       54|                  4|     1|        6|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = df6.orderBy(df6['DENIED'].desc())\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "|     AMAZON CORPORATE LLC|       50|                  1|     0|        1|\n",
      "|     ADOBE SYSTEMS INC...|       32|                  0|     1|        3|\n",
      "|               APPLE INC.|       20|                  0|     0|        0|\n",
      "|      CISCO SYSTEMS, INC.|       16|                  0|     1|        0|\n",
      "|              GOOGLE INC.|       15|                  6|     0|        0|\n",
      "|     WAL-MART ASSOCIAT...|       14|                  2|     0|        3|\n",
      "|             VMWARE, INC.|       12|                  0|     0|        0|\n",
      "|          EMC CORPORATION|       11|                  0|     0|        0|\n",
      "|     SEARS HOLDINGS MA...|        9|                  9|     0|        0|\n",
      "|     BECTON, DICKINSON...|        9|                  0|     0|        0|\n",
      "|     BURGER KING CORPO...|        8|                  1|     0|        0|\n",
      "|      GENENTECH USA, INC.|        8|                  1|     0|        2|\n",
      "|          ECOLAB USA INC.|        8|                  6|     0|        0|\n",
      "|            NETFLIX, INC.|        8|                  0|     0|        0|\n",
      "|     MICROSOFT CORPORA...|        8|                  4|     0|        0|\n",
      "|              DROGA5, LLC|        8|                  0|     0|        0|\n",
      "|        EPAM SYSTEMS, INC|        8|                  0|     0|        2|\n",
      "|       CVS PHARMACY, INC.|        8|                  3|     0|        1|\n",
      "|               ABBVIE INC|        8|                  0|     0|        0|\n",
      "|      GENERAL MILLS, INC.|        7|                  0|     0|        0|\n",
      "+-------------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.orderBy(df2['CERTIFIED'].desc())\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which `JOB_TITLE` got the highest number of certified visa applications?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.crosstab('JOB_TITLE', 'CASE_STATUS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------------+------+---------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|CERTIFIED-WITHDRAWN|DENIED|WITHDRAWN|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "|   OPERATIONS MANAGER|      206|                  9|    21|        6|\n",
      "|    MARKETING MANAGER|      191|                 11|    25|        9|\n",
      "|      GENERAL MANAGER|      128|                  2|    17|        3|\n",
      "| CHIEF EXECUTIVE O...|      127|                  3|    17|        5|\n",
      "|      PRODUCT MANAGER|       94|                 13|     0|        7|\n",
      "| BUSINESS DEVELOPM...|       91|                  2|     1|        2|\n",
      "| CHIEF OPERATING O...|       70|                  9|     6|        4|\n",
      "| PRODUCT MARKETING...|       47|                  8|     0|        4|\n",
      "| DIRECTOR OF OPERA...|       46|                  0|     3|        2|\n",
      "| SENIOR PRODUCT MA...|       45|                  5|     2|        0|\n",
      "|    MANAGING DIRECTOR|       34|                  2|     3|        0|\n",
      "|            PRESIDENT|       33|                  2|     2|        1|\n",
      "|                  CEO|       31|                  0|     5|        1|\n",
      "| GENERAL AND OPERA...|       29|                  1|     7|        4|\n",
      "|     ACCOUNT DIRECTOR|       26|                  1|     2|        2|\n",
      "|   EXECUTIVE DIRECTOR|       24|                  0|     1|        0|\n",
      "| BUSINESS DEVELOPM...|       23|                  0|     3|        0|\n",
      "| ADVERTISING AND P...|       23|                  0|     4|        1|\n",
      "|      PROJECT MANAGER|       21|                  1|     2|        1|\n",
      "|       VICE PRESIDENT|       19|                  2|     1|        0|\n",
      "+---------------------+---------+-------------------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.orderBy(df3['CERTIFIED'].desc())\n",
    "df3.orderBy(df3['CERTIFIED'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on columns\n",
    "\n",
    "**Classify Application status for each job title as either `Certified` or `NON-CERTIFIED`.**\n",
    "\n",
    "Hint: For each row we can sum up the values of columns `CERTIFIED-WITHDRAWN` + `WITHDRAWN` + `DENIED` into one single column as `NON-CERTIFIED`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+--------------------------------------------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|((CERTIFIED-WITHDRAWN + WITHDRAWN) + DENIED)|\n",
      "+---------------------+---------+--------------------------------------------+\n",
      "|    MARKETING MANAGER|       67|                                          12|\n",
      "|        SALES MANAGER|      146|                                          21|\n",
      "| BUSINESS DEVELOPM...|       52|                                           8|\n",
      "| VP, BUSINESS DEVE...|        3|                                           2|\n",
      "| ADMINISTRATIVE SE...|        4|                                           2|\n",
      "| ADMINISTRATIVE SE...|       14|                                           5|\n",
      "|      PROJECT MANAGER|      215|                                          31|\n",
      "|   MARKETING DIRECTOR|       12|                                           3|\n",
      "| INTERNATIONAL OPE...|        1|                                           2|\n",
      "|      VP, ENGINEERING|        5|                                           1|\n",
      "| SENIOR PROJECT MA...|       13|                                           3|\n",
      "| SALES OPERATIONS ...|        5|                                           1|\n",
      "| SOFTWARE DEVELOPM...|       51|                                           4|\n",
      "| TECHNICAL PROJECT...|       13|                                           7|\n",
      "|      LEAD CONSULTANT|      121|                                           2|\n",
      "|      GENERAL MANAGER|        5|                                           1|\n",
      "|      ACCOUNT MANAGER|       19|                                           2|\n",
      "|    SPECIALIST MASTER|       11|                                           1|\n",
      "|              MANAGER|      132|                                           9|\n",
      "|      PRODUCT MANAGER|       54|                                          11|\n",
      "+---------------------+---------+--------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5 = df6.select(df6['JOB_TITLE_CASE_STATUS'], df6['CERTIFIED'], df6['CERTIFIED-WITHDRAWN']+df6['WITHDRAWN']+df6['DENIED'])\n",
    "\n",
    "df6.select(df6['JOB_TITLE_CASE_STATUS'], df6['CERTIFIED'], df6['CERTIFIED-WITHDRAWN']+df6['WITHDRAWN']+df6['DENIED']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7 = df5.select('JOB_TITLE_CASE_STATUS', 'CERTIFIED', col(\"((CERTIFIED-WITHDRAWN + WITHDRAWN) + DENIED)\").alias(\"NON-CERTIFIED\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------+-------------+\n",
      "|JOB_TITLE_CASE_STATUS|CERTIFIED|NON-CERTIFIED|\n",
      "+---------------------+---------+-------------+\n",
      "|    MARKETING MANAGER|       67|           12|\n",
      "|        SALES MANAGER|      146|           21|\n",
      "| BUSINESS DEVELOPM...|       52|            8|\n",
      "| ADMINISTRATIVE SE...|        4|            2|\n",
      "| VP, BUSINESS DEVE...|        3|            2|\n",
      "| ADMINISTRATIVE SE...|       14|            5|\n",
      "|      PROJECT MANAGER|      215|           31|\n",
      "|   MARKETING DIRECTOR|       12|            3|\n",
      "| INTERNATIONAL OPE...|        1|            2|\n",
      "| SENIOR PROJECT MA...|       13|            3|\n",
      "| SOFTWARE DEVELOPM...|       51|            4|\n",
      "| DIRECTOR OF MARKE...|        9|            1|\n",
      "|      ACCOUNT MANAGER|       19|            2|\n",
      "| TECHNICAL PROJECT...|       13|            7|\n",
      "| SALES OPERATIONS ...|        5|            1|\n",
      "|      GENERAL MANAGER|        5|            1|\n",
      "|    SPECIALIST MASTER|       11|            1|\n",
      "|              MANAGER|      132|            9|\n",
      "|      LEAD CONSULTANT|      121|            2|\n",
      "|      PRODUCT MANAGER|       54|           11|\n",
      "+---------------------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the total number of `CERTIFIED` and `NON-CERTIFIED` applications in your dataframe.**\n",
    "\n",
    "Hint: Use aggregation function like sum() to compute total number in each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4273.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_certified = float(df7.groupBy().sum('CERTIFIED').collect()[0][0])\n",
    "total_certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "727.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarly, calculate total number of NON-CERTIFIED applications.\n",
    "\n",
    "total_non_certified = float(df7.groupBy().sum('NON-CERTIFIED').collect()[0][0])\n",
    "total_non_certified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s the problem: I have a Python function that iterates over my data, but going through each row in the dataframe takes several days. If I have a computing cluster with many nodes, how can I distribute this Python function in PySpark to speed up this process — maybe cut the total time down to less than a few hours — with the least amount of work?\n",
    "\n",
    "In other words, how do I turn a Python function into a Spark user defined function, or UDF?\n",
    "\n",
    "<img src = \"../images/dataframe.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we make use of User Defined Functions on our Dataframes?\n",
    "\n",
    "Recall, what was the use of `map()` and `flatMap()` methods when we were operating on our RDDs. Basically these help us to apply the user defined functions on each partition of our RDD.\n",
    "\n",
    "Similarly, spark allow us to operate on dataframe using our custom functions.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. Define your custom function\n",
    "2. Register UDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def share(s):\n",
    "  return (s / total_certified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering a UDF\n",
    "\n",
    "- PySpark UDFs work in a similar way as the pandas .map() and .apply() methods for pandas series and dataframes. If I have a function that can use values from a row in the dataframe as input, then I can map it to the entire dataframe. The only difference is that with PySpark UDFs we have to specify the output data type.\n",
    "\n",
    "- As long as the python function’s output has a corresponding data type in Spark, it can be turned into a UDF. When registering UDFs, we have to specify the data type using the types from pyspark.sql.types. All the types supported by PySpark [can be found here](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types).\n",
    "\n",
    "- udf(): Returns a **UDFRegistration** for UDF registration.\n",
    "\n",
    "- register(name, f, returnType=StringType): Registers a python function (including lambda function) as a **UDF** so it can be used in SQL statements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4273.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "sqlContext.udf.register(\"Employershare\", share)\n",
    "\n",
    "print total_certified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|CERTIFIED|\n",
      "+-------------------------+---------+\n",
      "|     TEMPLE UNIVERSITY...|      2.0|\n",
      "|     SAMSUNG ELECTRONI...|      4.0|\n",
      "|     MERCURY INSURANCE...|      0.0|\n",
      "|              ACCUEN INC.|      1.0|\n",
      "|        AKDY IMPORTS, LLC|      1.0|\n",
      "|           SAFEGRAPH INC.|      2.0|\n",
      "|     PUMA NORTH AMERIC...|      1.0|\n",
      "|             CYIENT, INC.|      1.0|\n",
      "|     THE SHERWIN-WILLI...|      3.0|\n",
      "|     ITG SOFTWARE SOLU...|      0.0|\n",
      "|             ANTHEM, INC.|      3.0|\n",
      "|     GAVS TECHNOLOGIES...|      1.0|\n",
      "|     GLOBAL TRAVEL SOL...|      1.0|\n",
      "|     INTERACTIVE BROAD...|      4.0|\n",
      "|            POPSUGAR INC.|      1.0|\n",
      "|       BRIGHT MARKET, LLC|      1.0|\n",
      "|     NOBLE DRILLING SE...|      1.0|\n",
      "|               PROJECT:TF|      1.0|\n",
      "|           PEOPLEASE, LLC|      1.0|\n",
      "|              AASONN, LLC|      1.0|\n",
      "+-------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7 = df2.select(df2.EMPLOYER_NAME_CASE_STATUS, df2.CERTIFIED.cast(\"float\"))\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "share_udf = udf(share, FloatType())\n",
    "df8 = df7.select(\"EMPLOYER_NAME_CASE_STATUS\", share_udf(df7.CERTIFIED).alias(\"%share\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|      %share|\n",
      "+-------------------------+------------+\n",
      "|     TEMPLE UNIVERSITY...|4.6805522E-4|\n",
      "|     SAMSUNG ELECTRONI...|9.3611045E-4|\n",
      "|     MERCURY INSURANCE...|         0.0|\n",
      "|              ACCUEN INC.|2.3402761E-4|\n",
      "|        AKDY IMPORTS, LLC|2.3402761E-4|\n",
      "|           SAFEGRAPH INC.|4.6805522E-4|\n",
      "|     PUMA NORTH AMERIC...|2.3402761E-4|\n",
      "|             CYIENT, INC.|2.3402761E-4|\n",
      "|     THE SHERWIN-WILLI...| 7.020828E-4|\n",
      "|     ITG SOFTWARE SOLU...|         0.0|\n",
      "|             ANTHEM, INC.| 7.020828E-4|\n",
      "|     GAVS TECHNOLOGIES...|2.3402761E-4|\n",
      "|     GLOBAL TRAVEL SOL...|2.3402761E-4|\n",
      "|     INTERACTIVE BROAD...|9.3611045E-4|\n",
      "|            POPSUGAR INC.|2.3402761E-4|\n",
      "|       BRIGHT MARKET, LLC|2.3402761E-4|\n",
      "|     NOBLE DRILLING SE...|2.3402761E-4|\n",
      "|               PROJECT:TF|2.3402761E-4|\n",
      "|           PEOPLEASE, LLC|2.3402761E-4|\n",
      "|              AASONN, LLC|2.3402761E-4|\n",
      "+-------------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here’s a small gotcha** — because Spark UDF doesn’t convert integers to floats, unlike Python function which works for both integers and floats, a Spark UDF will return a column of NULLs if the input data type doesn’t match the output data type, as in the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "share_integer_udf = udf(share, IntegerType())\n",
    "df9 = df7.select(\"EMPLOYER_NAME_CASE_STATUS\", share_integer_udf(df7.CERTIFIED).alias(\"%share\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|EMPLOYER_NAME_CASE_STATUS|%share|\n",
      "+-------------------------+------+\n",
      "|     TEMPLE UNIVERSITY...|  null|\n",
      "|     SAMSUNG ELECTRONI...|  null|\n",
      "|     MERCURY INSURANCE...|  null|\n",
      "|              ACCUEN INC.|  null|\n",
      "|        AKDY IMPORTS, LLC|  null|\n",
      "|           SAFEGRAPH INC.|  null|\n",
      "|     PUMA NORTH AMERIC...|  null|\n",
      "|             CYIENT, INC.|  null|\n",
      "|     THE SHERWIN-WILLI...|  null|\n",
      "|     ITG SOFTWARE SOLU...|  null|\n",
      "|             ANTHEM, INC.|  null|\n",
      "|     GAVS TECHNOLOGIES...|  null|\n",
      "|     GLOBAL TRAVEL SOL...|  null|\n",
      "|     INTERACTIVE BROAD...|  null|\n",
      "|            POPSUGAR INC.|  null|\n",
      "|       BRIGHT MARKET, LLC|  null|\n",
      "|     NOBLE DRILLING SE...|  null|\n",
      "|               PROJECT:TF|  null|\n",
      "|           PEOPLEASE, LLC|  null|\n",
      "|              AASONN, LLC|  null|\n",
      "+-------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Spark Dataframe to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'EMPLOYER_NAME_CASE_STATUS', u'%share'], dtype='object')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = df9.toPandas()\n",
    "pandas_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: SQL Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a temproary table view from a Dataframe, which can be further used to perform SQL queries on the data. In part 1, we saw operations using Dataframes. We will pick the same dataset and perform some basic SQL queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a table out of `df2`**\n",
    "\n",
    "Hint: We can use `registerTempTable(name)` method on any dataframe to create a table out of it.\n",
    "\n",
    "**`registerTempTable(name)`**: Registers this RDD as a temporary table using the given name.\n",
    "\n",
    "- The lifetime of this temporary table is tied to the SQLContext that was used to create this DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable(\"visa_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform SQL queries through Spark?\n",
    "\n",
    "TO perform SQL Queries we can `SparkSession.sql(sqlQuery)` where `sqlQuery` can be any valid sql query.\n",
    "\n",
    "- **`SparkSession.sql(sqlQuery)`**: **Returns a DataFrame** representing the result of the given query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visa = spark.sql(\"select * from visa_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check if the above query gave us the identical dataframe in the result!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(df_visa.collect()) == sorted(df.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+\n",
      "|       EMPLOYER_NAME|CERTIFIED_COUNT|\n",
      "+--------------------+---------------+\n",
      "|AMAZON CORPORATE LLC|             50|\n",
      "|ADOBE SYSTEMS INC...|             32|\n",
      "|          APPLE INC.|             20|\n",
      "| CISCO SYSTEMS, INC.|             16|\n",
      "|         GOOGLE INC.|             15|\n",
      "|WAL-MART ASSOCIAT...|             14|\n",
      "|        VMWARE, INC.|             12|\n",
      "|     EMC CORPORATION|             11|\n",
      "|BECTON, DICKINSON...|              9|\n",
      "|SEARS HOLDINGS MA...|              9|\n",
      "+--------------------+---------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top10 companies getting visa approval (for all the years)\n",
    "spark.sql(\"SELECT EMPLOYER_NAME, count(EMPLOYER_NAME) as CERTIFIED_COUNT FROM visa_table where CASE_STATUS = 'CERTIFIED' GROUP BY EMPLOYER_NAME order by CERTIFIED_COUNT desc\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|           JOB_TITLE|Approved|\n",
      "+--------------------+--------+\n",
      "|  OPERATIONS MANAGER|     206|\n",
      "|   MARKETING MANAGER|     191|\n",
      "|     GENERAL MANAGER|     128|\n",
      "|CHIEF EXECUTIVE O...|     127|\n",
      "|     PRODUCT MANAGER|      94|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT JOB_TITLE, count(*) as Approved FROM visa_table where CASE_STATUS = 'CERTIFIED' GROUP BY JOB_TITLE order by Approved desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in our dataset the Job Title as `OPERATIONS MANAGER` has got highest number of approvals.\n",
    "\n",
    "**Let's find out the `EMPLOYER_NAME` having the highest number of operations manager getting visa approved.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|       EMPLOYER_NAME|Approved|\n",
      "+--------------------+--------+\n",
      "|     TAKETOURS, INC.|       3|\n",
      "|       SOLE COOL INC|       3|\n",
      "|PRINTRONIC CORPOR...|       2|\n",
      "|MARUTI MANAGEMENT...|       2|\n",
      "|JAMSAN HOTEL MANA...|       2|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT EMPLOYER_NAME,count(*) as Approved FROM visa_table where CASE_STATUS = 'CERTIFIED' AND JOB_TITLE ='OPERATIONS MANAGER' GROUP BY EMPLOYER_NAME order by Approved desc\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, find out the approved applications having the highest paid salaries.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+------------------+--------------------+----+------------------+-----------+\n",
      "|          businesses|   wage|          SOC_NAME|           JOB_TITLE|YEAR|FULL_TIME_POSITION|CASE_STATUS|\n",
      "+--------------------+-------+------------------+--------------------+----+------------------+-----------+\n",
      "|BANDY CANYON RANC...|99986.0|  CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|2016|                 Y|  CERTIFIED|\n",
      "|CALIFORNIA GALVAN...|99986.0|  CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|2016|                 Y|  CERTIFIED|\n",
      "|         LOMICS, LLC|99986.0|  CHIEF EXECUTIVES|                 CEO|2016|                 Y|  CERTIFIED|\n",
      "|UC UNIVERSITY HIG...|99986.0|  CHIEF EXECUTIVES|CHIEF FINANCIAL O...|2016|                 Y|  CERTIFIED|\n",
      "|         SUN BUM LLC|99986.0|  CHIEF EXECUTIVES|CHIEF EXECUTIVE O...|2016|                 Y|  CERTIFIED|\n",
      "|THE STUDENT LOAN ...|99972.0|MARKETING MANAGERS|     PRODUCT MANAGER|2016|                 Y|  CERTIFIED|\n",
      "|SEARS HOLDINGS MA...|99972.0|MARKETING MANAGERS|MANAGER, BUSINESS...|2016|                 Y|  CERTIFIED|\n",
      "|SEARS HOLDINGS MA...|99972.0|MARKETING MANAGERS|   DIRECTOR, PRICING|2016|                 Y|  CERTIFIED|\n",
      "|SEARS HOLDINGS MA...|99972.0|MARKETING MANAGERS|DIRECTOR, PROGRAM...|2016|                 Y|  CERTIFIED|\n",
      "|THE STUDENT LOAN ...|99972.0|MARKETING MANAGERS|     PRODUCT MANAGER|2016|                 Y|  CERTIFIED|\n",
      "+--------------------+-------+------------------+--------------------+----+------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT EMPLOYER_NAME as businesses, PREVAILING_WAGE as wage, SOC_NAME, JOB_TITLE, YEAR, FULL_TIME_POSITION, CASE_STATUS  FROM visa_table where CASE_STATUS ='CERTIFIED' order by PREVAILING_WAGE desc\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, determine maximum salaries by `JOB_TITLE` for FULL TIME POSITIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|           JOB_TITLE|Max_Salary|\n",
      "+--------------------+----------+\n",
      "|CHIEF EXECUTIVE O...|   99986.0|\n",
      "|CHIEF EXECUTIVE O...|   99986.0|\n",
      "|CHIEF FINANCIAL O...|   99986.0|\n",
      "|                 CEO|   99986.0|\n",
      "|     PRODUCT MANAGER|   99972.0|\n",
      "|DIRECTOR, BUSINES...|   99972.0|\n",
      "|   DIRECTOR, PRICING|   99972.0|\n",
      "|DIRECTOR, PROGRAM...|   99972.0|\n",
      "|MANAGER, BUSINESS...|   99972.0|\n",
      "|PLASTICS MOVEMENT...|   99819.0|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT JOB_TITLE ,MAX(PREVAILING_WAGE) as Max_Salary FROM visa_table where CASE_STATUS ='CERTIFIED' AND  FULL_TIME_POSITION ='Y' GROUP BY JOB_TITLE ORDER BY Max_Salary DESC\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
